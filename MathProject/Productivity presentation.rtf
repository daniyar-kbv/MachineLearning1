{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww8060\viewh18000\viewkind0
\deftab560
\pard\pardeftab560\partightenfactor0

\f0\b\fs40 \cf0 Presentation Math Concepts\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f1\b0\fs26 \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 We also tried combining different feature selection methods \
(such as Pearson correlation, mutual information, recursive feature elimination and embedded feature selectors of Linear regressor, random forest and lgbm)\
Using the features selected by these six methods together allowed us to get the 0.01735 mean squared error, which is worse than we got using all the columns\
\
Here is the output of the method of selecting features that I have just mentioned. Each submethod returned group of features that impacts the result most\uc0\u8232 We tried fitting and predicting with different number of features, but the results weren\'92t satisfying\u8232 So training the model with all the columns were giving the best results yet\
\
After that me and my team tried to use grid search to tune the Random Forest Regressor\'92s hyper_parameters for better results\uc0\u8232 (The number of estimators, max depth, minimum number of samples for node to split and \'85, number of features for each tree and bootstrap parameters) \u8232 Unfortunately results did not satisfy as well\u8232 \u8232 As you can see these are the 8 models that we build with different feature selection approaches and diffrerent data encoding and normalization  methods\u8232 The improvements that we achieved are not huge, but still important\
\
In conclusion, describing the approach with the the best accuracy, it includes the Dummy encoding of categorical features, standard scaling of values for normalization, RandomForestRegressor with 50 trees and default other parameters and using all the features, that gave our team good 0.016 MSE\
\
now Rishabh will tell you further point\uc0\u8232 \u8232 \
}